{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1199f972-fe9c-4528-b76c-3f39438b0b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 05:45:58,272 : Line: 117 - Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n",
      "2022-02-09 05:45:58.268459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.271667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.271832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.272033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.273245: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-09 05:45:58.273566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.273732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.273881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.574383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.574537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.574656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 05:45:58.574757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5764 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from re import I\n",
    "import wandb\n",
    "import datetime\n",
    "import pdb\n",
    "from plotly import express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from logging import info, debug\n",
    "from tkinter import W\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import pprint\n",
    "from models.utils import show_progress\n",
    "import numpy as np\n",
    "from logging import log, info, debug\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import sys\n",
    "from models.models import build_resnet, build_siamese_autoencoder\n",
    "from losses import DiceLoss\n",
    "from metrics import DiceMetric\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "sys.path.append('/home/aadi/projects/pgrad-thesis/src/models')\n",
    "sys.path.append('/home/aadi/projects/pgrad-thesis/data')\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "io.use_plugin('pil')\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35eb880e-f199-4134-9add-ef89c4c60eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(img):\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def decode_grey(img):\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def decode_rgb(img):\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_path_rgb(fp):\n",
    "    img = tf.io.read_file(fp)\n",
    "    img = decode_rgb(img)\n",
    "    img = _normalize(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_path_grey(fp):\n",
    "    img = tf.io.read_file(fp)\n",
    "    img = decode_grey(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def build_data_horizontal_separate(batch_size, take, buffer_size=1000):\n",
    "\n",
    "    def train_gen(split='train', data_path='../data/'):\n",
    "        path = data_path + split\n",
    "        for t1, t2, l in zip(sorted(os.listdir(path+'/time1')), sorted(os.listdir(path+'/time2')), sorted(os.listdir(path+'/label'))):\n",
    "            # get full paths\n",
    "\n",
    "            t1 = process_path_rgb(f'../data/{split}/time1/' + t1)\n",
    "            t2 = process_path_rgb(f'../data/{split}/time2/' + t2)\n",
    "            l = process_path_grey(f'../data/{split}/label/' + l)\n",
    "\n",
    "            yield (t1, t2), l\n",
    "\n",
    "    def val_gen(split='val', data_path='../data/'):\n",
    "        path = data_path + split\n",
    "        for t1, t2, l in zip(sorted(os.listdir(path+'/time1')), sorted(os.listdir(path+'/time2')), sorted(os.listdir(path+'/label'))):\n",
    "            # get full paths\n",
    "\n",
    "            t1 = process_path_rgb(f'data/{split}/time1/' + t1)\n",
    "            t2 = process_path_rgb(f'data/{split}/time2/' + t2)\n",
    "            l = process_path_grey(f'data/{split}/label/' + l)\n",
    "\n",
    "            yield (t1, t2), l\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_generator(\n",
    "        train_gen, output_types=((tf.float32, tf.float32), tf.uint8))\n",
    "    val_ds = tf.data.Dataset.from_generator(\n",
    "        val_gen, output_types=((tf.float32, tf.float32), tf.uint8))\n",
    "\n",
    "    train_batches = (\n",
    "        train_ds\n",
    "        .cache()\n",
    "        .shuffle(buffer_size)\n",
    "        .batch(batch_size)\n",
    "        .take(take)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "    val_batches = (\n",
    "        val_ds\n",
    "        .cache()\n",
    "        .shuffle(buffer_size)\n",
    "        .batch(batch_size)\n",
    "        .take(take)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "    return train_batches, val_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add5ea9e-1372-434b-a93b-ff83b1db23d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left (InputLayer)               [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 16) 208         left[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 16) 1040        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 16) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 16) 1040        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "right (InputLayer)              [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 32) 2080        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 256, 16) 208         right[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 32) 4128        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 256, 16) 1040        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 128, 128, 16) 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 32)   4128        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 16) 1040        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   8256        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 2080        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 64)   16448       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 32) 4128        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 64)   16448       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 32)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 32)   4128        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 64)   16448       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   8256        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 128)  32896       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 64)   16448       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 128)  65664       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 64)   16448       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  65664       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 64)   16448       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 128)  32896       conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  65664       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 128)  65664       conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 16, 16, 128)  65664       conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 128)  65664       conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 128)  0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 32, 32, 128)  0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 32, 32, 128)  0           up_sampling2d[0][0]              \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 256)  0           subtract[0][0]                   \n",
      "                                                                 subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 32, 32, 256)  262400      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 128)  131200      conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 128)  65664       conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 32, 32, 64)   32832       conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 32, 32, 64)   16448       conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 32, 32, 64)   16448       conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 64)   0           conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 64, 64, 64)   0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 128)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 64, 64, 128)  65664       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 64, 64, 64)   32832       conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, 64, 64, 64)   16448       conv2d_transpose_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 64, 64, 32)   8224        conv2d_transpose_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 64, 64, 32)   4128        conv2d_transpose_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, 64, 64, 32)   4128        conv2d_transpose_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 32) 0           conv2d_transpose_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 128, 128, 32) 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 64) 0           up_sampling2d_2[0][0]            \n",
      "                                                                 subtract_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, 128, 128, 64) 16448       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DTran (None, 128, 128, 32) 8224        conv2d_transpose_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DTran (None, 128, 128, 16) 2064        conv2d_transpose_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DTran (None, 128, 128, 16) 1040        conv2d_transpose_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DTran (None, 128, 128, 16) 1040        conv2d_transpose_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 16) 0           conv2d_transpose_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 256, 256, 16) 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 32) 0           up_sampling2d_3[0][0]            \n",
      "                                                                 subtract_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DTran (None, 256, 256, 32) 4128        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DTran (None, 256, 256, 16) 2064        conv2d_transpose_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DTran (None, 256, 256, 1)  65          conv2d_transpose_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sigmoid (TFOpLambda)    (None, 256, 256, 1)  0           conv2d_transpose_21[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,291,713\n",
      "Trainable params: 1,291,713\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 25\n",
    "TRAIN_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE\n",
    "TEST_SIZE = 200\n",
    "\n",
    "train_data, val_data = build_data_horizontal_separate(\n",
    "        BATCH_SIZE, take=TRAIN_SIZE)\n",
    "\n",
    "model = build_siamese_autoencoder()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a790e3-9986-4cca-a134-83d2461efff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize METRICS for Tracking progress\n",
    "train_dice = DiceMetric()\n",
    "test_dice = DiceMetric()\n",
    "val_dice = DiceMetric()\n",
    "\n",
    "# initialize Dice LOSS for training step\n",
    "dice_loss = DiceLoss()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "#--------------------------------------TENSORFLOW TRAINING LOOP STEPS---------------------------------#\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, input, labels, optimizer):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input, training=True)\n",
    "        # Instantiating DiceLoss() is for passing into model constructor\n",
    "        print(logits.shape)\n",
    "        # loss = dice_loss(labels, logits)\n",
    "        loss = bce_loss(labels, logits)\n",
    "    info(loss)\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "\n",
    "    train_dice.update_state(labels, logits)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(model, input, labels):\n",
    "    logits = model(input, training=False)\n",
    "    val_dice.update_state(labels, logits)\n",
    "\n",
    "    loss = bce_loss(labels, logits)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, input, labels):\n",
    "    test_logits = model(input, training=False)\n",
    "    test_dice.update_state(labels, test_logits)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "\n",
    "# TensorBoard logging\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "val_log_dir = 'logs/gradient_tape/' + current_time + '/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "601bbc97-9e8e-431b-bc62-45fbabdd4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (x,y) in train_data:\n",
    "#     print(x)\n",
    "\n",
    "train_data, val_data = build_data_horizontal_separate(batch_size=10, take=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7006e1-295d-402b-b42c-c10c373416b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_data:\n",
    "    print(f'Max-y: {tf.math.reduce_max(y)}, Min-y: {tf.math.reduce_min(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee07ab8-4b10-4aaa-a491-41d1ef0f6783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/examples.git\n",
      "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-g8r77xmv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/examples.git /tmp/pip-req-build-g8r77xmv\n",
      "  Resolved https://github.com/tensorflow/examples.git to commit 97dd38c31090290543e1f829fbbaddd192e8cc19\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/aadi/miniconda3/envs/pgrad-thesis/lib/python3.8/site-packages (from tensorflow-examples===97dd38c31090290543e1f829fbbaddd192e8cc19-) (0.15.0)\n",
      "Requirement already satisfied: six in /home/aadi/miniconda3/envs/pgrad-thesis/lib/python3.8/site-packages (from tensorflow-examples===97dd38c31090290543e1f829fbbaddd192e8cc19-) (1.15.0)\n",
      "Building wheels for collected packages: tensorflow-examples\n",
      "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorflow-examples: filename=tensorflow_examples-97dd38c31090290543e1f829fbbaddd192e8cc19_-py3-none-any.whl size=268468 sha256=9a1c084a83c96b881fd5f9837b937f3dde4fabcc20df03ea9a23dae2d2c67958\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uk95prwr/wheels/4e/f5/c2/bfe75b834c9028b2529023bf74541c46ead531b513a8010d21\n",
      "\u001b[33m  WARNING: Built wheel for tensorflow-examples is invalid: Metadata 1.2 mandates PEP 440 version, but '97dd38c31090290543e1f829fbbaddd192e8cc19-' is not\u001b[0m\u001b[33m\n",
      "\u001b[0mFailed to build tensorflow-examples\n",
      "Installing collected packages: tensorflow-examples\n",
      "  Running setup.py install for tensorflow-examples ... \u001b[?25ldone\n",
      "\u001b[33m  DEPRECATION: tensorflow-examples was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25hSuccessfully installed tensorflow-examples-97dd38c31090290543e1f829fbbaddd192e8cc19-\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_516623/1828032368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tensorflow/examples.git\n",
    "\n",
    "import tensorflow as tf\n",
    "# was trying to see what format output should be\n",
    "import tensorflow_datasets as tfds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
