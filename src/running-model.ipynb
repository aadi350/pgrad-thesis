{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1199f972-fe9c-4528-b76c-3f39438b0b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 18:10:59,804 : Line: 117 - Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n",
      "2022-02-09 18:10:59.801029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:10:59.804072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:10:59.804196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:10:59.804356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:10:59.805610: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 18:10:59.805915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:10:59.806042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:10:59.806153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:11:00.088372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:11:00.088510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:11:00.088612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-09 18:11:00.088689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5828 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from re import I\n",
    "import wandb\n",
    "import datetime\n",
    "import pdb\n",
    "from plotly import express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from logging import info, debug\n",
    "from tkinter import W\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import pprint\n",
    "from models.utils import show_progress\n",
    "import numpy as np\n",
    "from logging import log, info, debug\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import sys\n",
    "from models.models import build_resnet, build_siamese_autoencoder\n",
    "from losses import DiceLoss\n",
    "from metrics import DiceMetric\n",
    "from matplotlib import pyplot as plt\n",
    "from params import *\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "sys.path.append('/home/aadi/projects/pgrad-thesis/src/models')\n",
    "sys.path.append('/home/aadi/projects/pgrad-thesis/data')\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "io.use_plugin('pil')\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35eb880e-f199-4134-9add-ef89c4c60eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left (InputLayer)               [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 16) 208         left[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 16) 1040        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 16) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 16) 1040        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "right (InputLayer)              [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 32) 2080        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 256, 16) 208         right[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 32) 4128        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 256, 16) 1040        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 128, 128, 16) 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 32)   4128        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 16) 1040        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   8256        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 2080        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 64)   16448       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 32) 4128        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 64)   16448       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 32)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 32)   4128        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 64)   16448       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   8256        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 128)  32896       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 64)   16448       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 128)  65664       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 64)   16448       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  65664       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 64)   16448       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 128)  32896       conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  65664       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 128)  65664       conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 16, 16, 128)  65664       conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 128)  65664       conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 128)  0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 32, 32, 128)  0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 32, 32, 128)  0           up_sampling2d[0][0]              \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 256)  0           subtract[0][0]                   \n",
      "                                                                 subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 32, 32, 256)  262400      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 128)  131200      conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 128)  65664       conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 32, 32, 64)   32832       conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 32, 32, 64)   16448       conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 32, 32, 64)   16448       conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 64)   0           conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 64, 64, 64)   0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 128)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 64, 64, 128)  65664       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 64, 64, 64)   32832       conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, 64, 64, 64)   16448       conv2d_transpose_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 64, 64, 32)   8224        conv2d_transpose_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 64, 64, 32)   4128        conv2d_transpose_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, 64, 64, 32)   4128        conv2d_transpose_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 32) 0           conv2d_transpose_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 128, 128, 32) 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 64) 0           up_sampling2d_2[0][0]            \n",
      "                                                                 subtract_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, 128, 128, 64) 16448       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DTran (None, 128, 128, 32) 8224        conv2d_transpose_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DTran (None, 128, 128, 16) 2064        conv2d_transpose_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DTran (None, 128, 128, 16) 1040        conv2d_transpose_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DTran (None, 128, 128, 16) 1040        conv2d_transpose_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 16) 0           conv2d_transpose_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 256, 256, 16) 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 32) 0           up_sampling2d_3[0][0]            \n",
      "                                                                 subtract_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DTran (None, 256, 256, 32) 4128        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DTran (None, 256, 256, 16) 2064        conv2d_transpose_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DTran (None, 256, 256, 2)  130         conv2d_transpose_20[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,291,778\n",
      "Trainable params: 1,291,778\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import build_data\n",
    "from models.models import build_siamese_autoencoder\n",
    "# help(build_data)\n",
    "\n",
    "train, val = build_data.build_data_horizontal_separate(25, 100)\n",
    "model = build_siamese_autoencoder()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a790e3-9986-4cca-a134-83d2461efff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize METRICS for Tracking progress\n",
    "train_dice = DiceMetric()\n",
    "test_dice = DiceMetric()\n",
    "val_dice = DiceMetric()\n",
    "\n",
    "# initialize Dice LOSS for training step\n",
    "dice_loss = DiceLoss()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "#--------------------------------------TENSORFLOW TRAINING LOOP STEPS---------------------------------#\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, input, labels, optimizer):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input, training=True)\n",
    "        # Instantiating DiceLoss() is for passing into model constructor\n",
    "        print(logits.shape)\n",
    "        # loss = dice_loss(labels, logits)\n",
    "        loss = bce_loss(labels, logits)\n",
    "    info(loss)\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "\n",
    "    train_dice.update_state(labels, logits)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(model, input, labels):\n",
    "    logits = model(input, training=False)\n",
    "    val_dice.update_state(labels, logits)\n",
    "\n",
    "    loss = bce_loss(labels, logits)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, input, labels):\n",
    "    test_logits = model(input, training=False)\n",
    "    test_dice.update_state(labels, test_logits)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50927f8a-6c06-4032-90f8-dd246092f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from cucim.skimage.transform import resize\n",
    "from distutils.debug import DEBUG\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from models.utils import show_progress\n",
    "import cv2 as cv\n",
    "import skimage\n",
    "from tqdm import tqdm\n",
    "from difference_functions import basic_subtract\n",
    "\n",
    "logging.basicConfig(level=DEBUG)\n",
    "TRAIN_DIR = 'data/train'\n",
    "VAL_DIR = 'data/val'\n",
    "TEST_DIR = 'data/test'\n",
    "\n",
    "\n",
    "def _normalize(images, input_mask):\n",
    "    before_image, after_image = images\n",
    "    before_image = tf.cast(before_image, tf.float32) / 255.0\n",
    "    after_image = tf.cast(after_image, tf.float32) / 255.0\n",
    "    input_mask -= 1\n",
    "    return (before_image, after_image), input_mask\n",
    "\n",
    "\n",
    "def difference_data(data_path, difference_function=None):\n",
    "    '''Assume three folders exist in data_path: time1, time2, label'''\n",
    "\n",
    "    if not difference_function:\n",
    "        difference_function = basic_subtract\n",
    "\n",
    "    assert 'differenced' not in os.listdir(\n",
    "        data_path), 'Differenced folder exists'\n",
    "    assert set(os.listdir(data_path)) == set(\n",
    "        ['time1', 'time2', 'label']), \"Folders not present OR too many folders\"\n",
    "\n",
    "    before_path = os.path.join(data_path, 'time1')\n",
    "    after_path = os.path.join(data_path, 'time2')\n",
    "    label_path = os.path.join(data_path, 'label')\n",
    "\n",
    "    assert len(os.listdir(before_path)) == len(os.listdir(after_path)\n",
    "                                               ), \"Length of before images different from length of after images\"\n",
    "\n",
    "    os.mkdir(os.path.join(data_path, 'differenced'))\n",
    "\n",
    "    before_image_list = os.listdir(before_path)\n",
    "    after_image_list = os.listdir(after_path)\n",
    "    label_image_list = os.listdir(label_path)\n",
    "\n",
    "    for i in tqdm(range(len(os.listdir(before_path)))):\n",
    "        before_image_path = os.path.join(\n",
    "            data_path, 'time1', before_image_list[i])\n",
    "        after_image_path = os.path.join(\n",
    "            data_path, 'time2', after_image_list[i])\n",
    "        label_image_path = os.path.join(\n",
    "            data_path, 'label', label_image_list[i])\n",
    "        before_img = skimage.io.imread(before_image_path)\n",
    "        after_img = skimage.io.imread(after_image_path)\n",
    "\n",
    "        difference = difference_function(before_img, after_img)\n",
    "        skimage.io.imsave(os.path.join(\n",
    "            *[data_path, 'differenced', f'{i}.png']), difference)\n",
    "\n",
    "\n",
    "def build_differenced_data(N=-1, batch_size=100, train=True, val=False, test=False):\n",
    "    '''Assumes differenced images are in data/train/differenced, data/val/differenced, data/test/differenced and loads into single tf.data.Dataset with labels,\n",
    "        returns tf.data.Dataset of (differenced, labels)\n",
    "    '''\n",
    "    autotune = tf.data.AUTOTUNE\n",
    "\n",
    "    if train:\n",
    "        main_dir = TRAIN_DIR\n",
    "    elif val:\n",
    "        main_dir = VAL_DIR\n",
    "    else:\n",
    "        main_dir = TEST_DIR\n",
    "\n",
    "    diff_list = os.listdir(main_dir + '/differenced')[:N]\n",
    "    label_list = os.listdir(main_dir + '/label')[:N]\n",
    "\n",
    "    diff_list = [np.array(Image.open(main_dir + '/differenced/' + fname))\n",
    "                 for fname in diff_list]\n",
    "    label_list = [np.array(Image.open(main_dir + '/label/' + fname))\n",
    "                  for fname in label_list]\n",
    "\n",
    "    diff_list = [tf.convert_to_tensor(\n",
    "        image, dtype=tf.float32) for image in diff_list]\n",
    "    label_list = [tf.convert_to_tensor(\n",
    "        image, dtype=tf.float32) for image in label_list]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((diff_list, label_list))\n",
    "\n",
    "    del diff_list\n",
    "    del label_list\n",
    "\n",
    "    return ds.batch(batch_size).prefetch(buffer_size=autotune)\n",
    "\n",
    "# Uses too much memory\n",
    "\n",
    "\n",
    "def _build_train_arrays(N=-1):\n",
    "    raise MemoryError(\n",
    "        f'{_build_train_arrays.__name__} exceeds Protobuf memory usage')\n",
    "    train_before_list = os.listdir(TRAIN_DIR + '/time1')[:N]\n",
    "    train_after_list = os.listdir(TRAIN_DIR + '/time2')[:N]\n",
    "    train_label_list = os.listdir(TRAIN_DIR + '/label')[:N]\n",
    "    before_train = np.array([np.array(Image.open(TRAIN_DIR + '/time1/' + fname))\n",
    "                            for fname in tqdm(train_before_list)])\n",
    "    after_train = np.array([np.array(Image.open(TRAIN_DIR + '/time2/' + fname))\n",
    "                            for fname in tqdm(train_after_list)])\n",
    "    label_train = np.array([np.array(Image.open(TRAIN_DIR + '/label/' + fname))\n",
    "                            for fname in tqdm(train_label_list)])\n",
    "\n",
    "    return before_train, after_train, label_train\n",
    "\n",
    "\n",
    "def _build_val_arrays(N=-1):\n",
    "    raise MemoryError(\n",
    "        f'{_build_val_arrays} exceeds Protobuf memory usage')\n",
    "    val_before_list = os.listdir(VAL_DIR + '/time1')[:N]\n",
    "    val_after_list = os.listdir(VAL_DIR + '/time2')[:N]\n",
    "    val_label_list = os.listdir(VAL_DIR + '/label')[:N]\n",
    "    before_val = np.array([np.array(Image.open(VAL_DIR + '/time1/' + fname))\n",
    "                           for fname in show_progress(val_before_list)])\n",
    "    after_val = np.array([np.array(Image.open(VAL_DIR + '/time2/' + fname))\n",
    "                          for fname in show_progress(val_after_list)])\n",
    "    label_val = np.array([np.array(Image.open(VAL_DIR + '/label/' + fname))\n",
    "                          for fname in show_progress(val_label_list)])\n",
    "\n",
    "    return before_val, after_val, label_val\n",
    "\n",
    "\n",
    "def build_diff_data_rgb(batch_size, N=-1, image_shape=None):\n",
    "    before_train, after_train, train_label = _build_train_arrays(N=N)\n",
    "    before_val, after_val, val_label = _build_val_arrays(N=N)\n",
    "\n",
    "    train_diff = np.array([basic_subtract(b, a)\n",
    "                           for (b, a) in zip(before_train, after_train)])\n",
    "    val_diff = np.array([basic_subtract(b, a)\n",
    "                         for (b, a) in zip(before_val, after_val)])\n",
    "\n",
    "    if image_shape:\n",
    "        # reshape if pretrained weights expect different input sizes\n",
    "        train_diff = [cv.resize(img, image_shape) for img in train_diff]\n",
    "        val_diff = [cv.resize(img, image_shape) for img in val_diff]\n",
    "        train_label = [cv.resize(img, image_shape) for img in train_label]\n",
    "        val_label = [cv.resize(img, image_shape) for img in val_label]\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (train_diff, train_label)).batch(batch_size=batch_size)\n",
    "    val_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (val_diff, val_label)).batch(batch_size=batch_size)\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def _normalize(img):\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def decode_grey(img):\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def decode_rgb(img):\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_path_rgb(fp):\n",
    "    img = tf.io.read_file(fp)\n",
    "    img = decode_rgb(img)\n",
    "    img = _normalize(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_path_bin(fp: str) -> tf.Tensor:\n",
    "    img = process_path_grey(fp)\n",
    "    img = binarize_label(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_path_grey(fp):\n",
    "    img = tf.io.read_file(fp)\n",
    "    img = decode_grey(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def binarize_label(t: tf.Tensor) -> tf.Tensor:\n",
    "    t = tf.where(t == 255, 1, 0)\n",
    "    t = tf.cast(t, dtype=tf.uint8)\n",
    "    return t\n",
    "\n",
    "\n",
    "def build_data_horizontal_separate(batch_size, take, buffer_size=1000):\n",
    "\n",
    "    def train_gen(split='train', data_path='../data/'):\n",
    "        path = data_path + split\n",
    "        for t1, t2, l in zip(sorted(os.listdir(path+'/time1')), sorted(os.listdir(path+'/time2')), sorted(os.listdir(path+'/label'))):\n",
    "            # get full paths\n",
    "\n",
    "            t1 = process_path_rgb(f'../data/{split}/time1/' + t1)\n",
    "            t2 = process_path_rgb(f'../data/{split}/time2/' + t2)\n",
    "            l = process_path_bin(f'../data/{split}/label/' + l)\n",
    "\n",
    "            yield (t1, t2), l\n",
    "\n",
    "    def val_gen(split='val', data_path='../data/'):\n",
    "        path = data_path + split\n",
    "        for t1, t2, l in zip(sorted(os.listdir(path+'/time1')), sorted(os.listdir(path+'/time2')), sorted(os.listdir(path+'/label'))):\n",
    "            # get full paths\n",
    "\n",
    "            t1 = process_path_rgb(f'../data/{split}/time1/' + t1)\n",
    "            t2 = process_path_rgb(f'../data/{split}/time2/' + t2)\n",
    "            l = process_path_bin(f'../data/{split}/label/' + l)\n",
    "\n",
    "            yield (t1, t2), l\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_generator(\n",
    "        train_gen, output_types=((tf.float32, tf.float32), tf.uint8))\n",
    "    val_ds = tf.data.Dataset.from_generator(\n",
    "        val_gen, output_types=((tf.float32, tf.float32), tf.uint8))\n",
    "\n",
    "    train_batches = (\n",
    "        train_ds\n",
    "        .cache()\n",
    "        .shuffle(buffer_size)\n",
    "        .batch(batch_size)\n",
    "        .take(take)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "    val_batches = (\n",
    "        val_ds\n",
    "        .cache()\n",
    "        .shuffle(buffer_size)\n",
    "        .batch(batch_size)\n",
    "        .take(take)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "    return train_batches, val_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "601bbc97-9e8e-431b-bc62-45fbabdd4049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.5058184, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3656359, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6815155, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5150565, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5537194, shape=(), dtype=float32)\n",
      "tf.Tensor(1.587725, shape=(), dtype=float32)\n",
      "tf.Tensor(1.7112675, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6290858, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6060952, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5044984, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5482565, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6204467, shape=(), dtype=float32)\n",
      "tf.Tensor(1.555047, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4933072, shape=(), dtype=float32)\n",
      "tf.Tensor(1.587381, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5827873, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5489775, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5929946, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5676515, shape=(), dtype=float32)\n",
      "tf.Tensor(1.7583501, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4065365, shape=(), dtype=float32)\n",
      "tf.Tensor(1.543359, shape=(), dtype=float32)\n",
      "tf.Tensor(1.713507, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5439044, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4814253, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 18:29:10.294541: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train, val = build_data_horizontal_separate(batch_size=25, take=100)\n",
    "loss_fn = DiceLoss() # tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "for (x, y) in train.take(25):\n",
    "    y_pred = model(x)\n",
    "    print(loss_fn(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7006e1-295d-402b-b42c-c10c373416b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_data:\n",
    "    print(f'Max-y: {tf.math.reduce_max(y)}, Min-y: {tf.math.reduce_min(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee07ab8-4b10-4aaa-a491-41d1ef0f6783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/examples.git\n",
      "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-g8r77xmv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/examples.git /tmp/pip-req-build-g8r77xmv\n",
      "  Resolved https://github.com/tensorflow/examples.git to commit 97dd38c31090290543e1f829fbbaddd192e8cc19\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/aadi/miniconda3/envs/pgrad-thesis/lib/python3.8/site-packages (from tensorflow-examples===97dd38c31090290543e1f829fbbaddd192e8cc19-) (0.15.0)\n",
      "Requirement already satisfied: six in /home/aadi/miniconda3/envs/pgrad-thesis/lib/python3.8/site-packages (from tensorflow-examples===97dd38c31090290543e1f829fbbaddd192e8cc19-) (1.15.0)\n",
      "Building wheels for collected packages: tensorflow-examples\n",
      "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorflow-examples: filename=tensorflow_examples-97dd38c31090290543e1f829fbbaddd192e8cc19_-py3-none-any.whl size=268468 sha256=9a1c084a83c96b881fd5f9837b937f3dde4fabcc20df03ea9a23dae2d2c67958\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uk95prwr/wheels/4e/f5/c2/bfe75b834c9028b2529023bf74541c46ead531b513a8010d21\n",
      "\u001b[33m  WARNING: Built wheel for tensorflow-examples is invalid: Metadata 1.2 mandates PEP 440 version, but '97dd38c31090290543e1f829fbbaddd192e8cc19-' is not\u001b[0m\u001b[33m\n",
      "\u001b[0mFailed to build tensorflow-examples\n",
      "Installing collected packages: tensorflow-examples\n",
      "  Running setup.py install for tensorflow-examples ... \u001b[?25ldone\n",
      "\u001b[33m  DEPRECATION: tensorflow-examples was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25hSuccessfully installed tensorflow-examples-97dd38c31090290543e1f829fbbaddd192e8cc19-\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_516623/1828032368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
